\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Sentiment Analysis with Sarcasm Detection\\
}

\author{\IEEEauthorblockN{Ryan Gross}
rgross4@gwmail.gwu.edu
\and
\IEEEauthorblockN{Sam Odle}
samodle@gwmail.gwu.edu
}

\maketitle

\begin{abstract}
The Sentiment Analysis with Sarcasm Detection project aims to outperform current state-of-the-art sentiment analysis systems by fixing a major shortcoming of current algorithms. 
Current sentiment systems fail to recognize sarcasm, often rendering sentiment scores that are not only inaccurate, but can be the opposite of the author’s actual intended sentiment. This shortcoming is readily apparent when companies use sentiment scores to automatically reply to comments, reviews, and tweets using chatbots. \\
In this experiment, we have combined several techniques for detecting sarcasm and averaged their analysis scores to better inform the final sentiment of the input text. While sentiment accuracy is a purely subjective metric, we will demonstrate results on passages that clearly demonstrate improvements over Google Cloud Platform’s (GCP) Natural Language sentiment analysis results for the same input text. 
This paper will outline the methodology for each sarcasm detection technique, talk to the strengths and weaknesses of each, and show testing results. Furthermore, it will convince the reader that prioritizing sarcasm detection is essential for sentiment analysis accuracy, especially in real world applications. 

\end{abstract}

\section{Introduction}

This paper’s primary purpose is to convince the reader through examples that today’s Sentiment Analysis systems produce flawed results when sarcasm is present because they do not take it into account when scoring input text. This paper will discuss why that presents problems to businesses of all sizes, again using real world examples. Additionally, the reader will see several potential methods to close this existing gap and will see results that will prove the additional detection of sarcasm improves Sentiment Analysis accuracy. \\
Today, Sentiment Analysis is used by companies all over the world to better understand and connect with their customers. For example, companies will run Sentiment Analysis services against Yelp reviews, Amazon reviews, tweets on Twitter that mention their product or company, and many more sources for customer feedback. Companies also use real-time sentiment analysis services like Contact Lens for Amazon Connect to better serve customers their staff is actively engaged with.\\
With the constant growth of online services, Sentiment Analysis has become an essential tool for customer service organizations.  There simply is too much data for employees to manually review, so Sentiment Analysis services automate the task of bucketing negative and positive reviews. A quick response is important because these reviews are public and will immediately impact other potential customers.  It is also a learning opportunity for the company on how to improve their product or better serve the customer. It may help them identify trends or potential issues that they themselves are unaware of. Identifying a poor review is also an opportunity for a customer representative to assist the customer or provide a refund, which can sometimes lead to the customer improving their review of the product.\\

\begin{figure}[htbp]
\centerline{\includegraphics[width=9cm]{image1.png}}
\caption{L.L. Bean Responds to Negative Review}
\label{fig}
\end{figure}

Notice how in Figure 1, above, a customer representative was able to reach out to the customer to apologize and rectify the situation. Understanding customer sentiment also gives companies an opportunity to engage with customers and thank them for their patronage. In the Yelp review (Figure 2), you can see a small business owner thanking a customer for their review.  Even in situations like this one where online stores or rating services provide quantitative ranking systems (e.g. stars), running Automated Sentiment Analysis against reviews still provides useful metrics on what customers like and dislike about a product line.\\

\begin{figure}[htbp]
\centerline{\includegraphics[width=9cm]{image 2.png}}
\caption{Business Responds to Positive Yelp Review}
\label{fig2}
\end{figure}

Social media presents a harder challenge for Sentiment Analysis systems as there is no mathematical representation for a customer’s content or discontent with their product or brand. This is where Sentiment Analysis is the most crucial for a company, especially one with a global footprint. In order to edge out opponents in customer service and retention, large companies have employed chatbots to automatically respond to customers on social media. This typically results in a short pleasant response letting the customer know that the company appreciates them.\\

\begin{figure}[htbp]
\centerline{\includegraphics[width=9cm]{image4.png}}
\caption{AmericanAir Thanks Customer on Twitter}
\label{fig3}
\end{figure}


However, when Sentiment Analysis mischaracterizes the sentiment of the customer by not detecting sarcasm, it can have the exact opposite effect on customer retention. Today, many state-of-the-art Sentiment Analysis services continue to serve results that do not detect sarcasm. This can be easily seen using Google Cloud’s Natural Language demo. In Figure 5 below, Google’s Sentiment system produces a score that is very positive for the statement “I love being ignored!”. Any human reader would immediately detect that this statement carries a strong negative sentiment, because no one likes to be ignored. \\
In the next section on related project works we will discuss what research efforts have been undertaken to address the current gap.\\

\section{Related Project Works}
There are many research papers on the topic of detecting sarcasm in Natural Language Processing (NLP). Much of our inspiration came from Kaggle, a machine learning and data science social networking community. In particular, this page (https://www.kaggle.com/danofer/sarcasm) hosted the training data we used and linked to various other Kaggle member’s projects we looked at to come up with our approaches. One method in particular that caught our attention is word-embedding. This is a cutting edge NLP approach that allows algorithms to find similar words based on vector math. \\
Another site we found after we had started on our project is http://www.thesarcasmdetector.com/. This web site hosts an application that is similar to our own. In reading the author's approach, they attempted three main techniques, sentiment deviation, n-grams, and general statics on most sarcastic topics. While we had already built our sentiment deviation algorithm before discovering this site, we decided to build our own n-gram algorithm after reading that this approach was the most successful for the author.\\
Our sentiment deviation algorithm has better success than our n-grams algorithm, which is the opposite of the author. There are additional approaches in papers such as looking at punctuation, capital letters, emojis, and just in general throwing machine learning at the problem without focusing on any particular feature. Many different approaches could have been tried, but the ones stated above are the approaches that seemed the most promising to our group. With additional time we could try more algorithms to see how they stack up against ours.\\

\section{Solution \& Analysis}
Our solution is an API endpoint where the user submits a piece of text and gets back a sentiment analysis score. Our endpoint internally makes calls to our sarcasm detection algorithms, which enable it to produce better results than current state-of-the-art sentiment analysis services in many cases. Additionally, we built a web page for demonstration purposes that allows a user to enter text, submit it to the API, and see whether or not each sarcasm detection algorithm detected sarcasm, as well as the before and after sentiment scores. The “before” sentiment score is Google Cloud’s Natural Language Service Sentiment Analysis score. The “after” sentiment score is what our algorithm produced so users can see the difference that sarcasm detection makes in sentiment analysis.\\
\subsection{Segmented Sentiment Deviation}
The Segmented Sentiment Deviation algorithm is based on the premise that sarcastic sentences are strongly emotional sentences where the sentiment of individual words fluctuate drastically and quickly in comparison to non-sarcastic sentences. The implementation of this algorithm utilizes Google’s Sentiment Analysis services by taking the input text, splitting it into individual words, and then feeding each word to Google’s service. This not only does the word based sentiment analysis for us, but it also returns a normalized sentiment score. After receiving the sentiment score of each word, the algorithm looks for strongly negative sentiment words and looks for strongly positive neighbors. We choose to consider neighbors up to five words away based on some preliminary analysis of our sarcasm dataset. If the algorithm finds a positive neighbor near the negative word, it flags the sentence as possibly being sarcastic. To reduce errors, we have defined strongly negative to mean a score of -0.5, and strongly positive to mean a score of 0.75, on a scale of -1 to 1. Thus the positive word has to be over the top, something like love, happy, excited, or overjoyed. \\
An example sentence that would trigger this algorithm is, “I love being ignored!” This is a great example because love is one of the most strongly positive words, and ignored is very negative. This very quick fluctuation in sentiment is a common tool that an author can employ to tell the reader that they feel exactly the opposite of what they have said. In fact the non-sarcastic version of this statement would likely use “hate” instead of “love.”  The sentiment deviation algorithm was tested on the same dataset as all the other algorithms. Each algorithm used a corpus of both sarcastic and non-sarcastic comment mined from Reddit https://www.kaggle.com/danofer/sarcasm?select=train-balanced-sarcasm.csv. This algorithm was 87.8\% accurate on non-sarcastic text, 16.8\% on sarcastic text, and overall was 52.3\% accurate on the combined corpus. This is somewhat low, but any score above 50\% overall is an improvement over what exists today. In all our algorithms we favored a conservative approach, giving up sarcasm accuracy to ensure that non-sarcasm accuracy didn’t fall below acceptable levels. \\
The biggest con of this algorithm is that it is an expert system as opposed to a machine learning system, and therefore it can not improve over time. Another issue we noticed is that Google’s sentiment scores on individual words doesn’t always seem to make sense. For example, using the following sarcastic comment from our dataset, “I guess there is *one* good thing for alcoholism gripping our youth.” the algorithm was unable to detect the presence of sarcasm. After further review it is because Google’s Sentiment Analysis service gave the word “alcoholism” a positive 0.3 score, instead of the strongly negative score we feel it deserves. “good” and “alcoholism” should have been contradictory scores triggering our algorithm, but it is out of scope in the project to start sentiment analysis from scratch. The biggest pro of this algorithm is that it can actually identify the words in the sentence that are sarcastic. In the previous example “love” was the sarcastic part of the sentence. In our demo we highlight the sarcastic word in red when this algorithm detects sarcasm. This is the only algorithm that we employed that is able to narrow sarcasm down to individual words. \\
\subsection{Sarcastic N-Grams}
The Sarcastic n-grams algorithm aims to statistically determine the most common set of words found in sarcastic remarks. An n-gram is simply a phrase made up of n words. Thus a bigram or 2-gram is two words, a trigram or 3-gram is three words, and the number can grow to any number n. In our algorithm we ended up choosing 4-grams and 5-grams. After trying many different n values, we discovered that higher n’s were less error prone. Higher n’s also led to a lower sarcasm detection rate, but a bigram or trigram had unacceptable accuracy scores on non-sarcastic sentences because the bi and tri grams were very generic. The most common n-grams were mined from the Reddit corpus https://www.kaggle.com/danofer/sarcasm?select=train-balanced-sarcasm.csv. Here are some sample 4-grams and 5-grams that were found to be most common for sarcastic sentences: “Am I the only”, “I would like to”, “I just want to”, “I would love to”, “This is what happens when”, “but at the same time”, “many many many many many.” \\
To reduce the number of false positives, the algorithm subtracts any n-grams that are also most common in non-sarcastic text, so that the final list contains the most common sarcasm n-grams that are not also common in non-sarcastic text. With this final list, the algorithm checks incoming text against the list to see if the input text contains a sarcasm n-gram. At a high level this algorithm aims to find an English vocabulary for sarcasm and see if that vocabulary exists in a new piece of material. This algorithm was tested the same way as all the other algorithms, using the Reddit corpus. It is worth mentioning that it is bad practice to train and test on the same data, but we do not have an alternative data source at this time. A future improvement for this algorithm would be to run it against a more varied data source. The n-gram algorithm was 90.2\% accurate on non-sarcastic text, 9.9\% on sarcastic text, and overall was 50.04\% accurate on the combined corpus. The overall accuracy of this algorithm is poor considering 50\% is the break even value for it to even be worth using. However, we believe that this algorithm shows promise in finding a sarcasm vocabulary and really just needs better training data to be effective. One main benefit of this algorithm is that it will improve with improved training data.\\
\subsection{Semantic Similarity}
The Semantic Similarity algorithm is based on the premise that sarcastic content will be more similar in meaning and semantic content to other sarcastic content than to non-sarcastic content.  The implementation of this algorithm involved using Word2Vec generated embeddings to determine the cosine similarities between an input text string and both a sarcastic and non-sarcastic corpus.  Word2Vec is a collection of ML algorithms published by Google (https://code.google.com/archive/p/word2vec/). Word2Vec uses neural networks to learn word associations and generate word embeddings.  These embeddings can then be used to calculate the ‘distance’ between two text strings or documents using several techniques, each of which provides a measure of similarity between the text.  \\
We experimented with two techniques for calculating distance: Word Mover’s Distance and Cosine Similarity.  Once the Word2Vec vector space has been generated, Cosine Similarity is a straightforward Euclidean distance calculation.  Word Mover’s Distance, as outlined in this paper (http://proceedings.mlr.press/v37/kusnerb15.pdf), calculates the minimum Earth Mover’s Distance between each word in each of the texts.  Due to the resource intensity of a WMD calculation on large documents like ours, it would not be a viable API candidate (a single calculation takes several minutes), so we focused on Cosine Similarity.  \\ 
We used the Kaggle Reddit dataset to create a baseline sarcastic corpus of approximately 480,000 sarcastic comments and a baseline non-sarcastic corpus of approximately 480,000 comments.  The results in the table below were calculated using a test dataset comprising 20,000 sarcastic comments and 20,000 non-sarcastic comments.  Our algorithm calculated the Euclidean distance between each input and each of the two baseline corpora, and found that on average just over 60\% of the time the algorithm is able to correctly identify both sarcastic and non-sarcastic comments.\\
These findings validate the initial premise that sarcastic content does in general bear more semantic similarity to non-sarcastic content, and vice versa.  While the overall accuracy is significantly higher than the other two approaches, the non-sarcastic accuracy is substantially lower, so this technique should not be used by itself to detect sarcasm. \\
One final key finding of note is that in cases where the algorithm correctly identified the input text, the average difference between the sarcastic and non-sarcastic corpora is 25\% larger than when the algorithm incorrectly identified the input.  This difference indicates it may be possible to weed out inputs that are likely to be incorrect, thus improving the accuracy of the output and relegating the filtered outputs to other methods if our sarcasm detection were implemented in a more elaborate ensemble manner.  We experimented with multiple versions of filtering out values where the difference in the distances is smallest and initial results indicate there is not a direct correlation between the difference in the distance between the corpora and the likelihood of an accurate identification.\\

\section{Summary \& Future Work}
In conclusion, in the test environment using Reddit data where 50\% of the inputs are sarcastic, the Sentiment Analysis with Sarcasm Detection project outperformed Google’s API in percent of inputs correctly identified.  Further work should be done before the techniques outlined in this paper are applied to any real world situations because the percentage of sarcastic comments is likely to be much lower. (based on Reddit data only 1/10 of comments are sarcastic).  Future work could include: 
\begin{itemize}
\item \textbf{Creating Labeled Product Review Dataset.} Ideally we would’ve used a dataset with product reviews labeled as sarcastic.  Existing product review sets are unlabeled.
\item \textbf{N-Grams: Reapply Best Practices.}  Better training data should improve results, we could also more closely reapply the algorithm we drew inspiration from.
\item \textbf{Sentiment Similarity: Filter Candidate Inputs Likely To Fail.}  Certain distances seem more likely to be incorrect, further experimentation could prove this out.
\item \textbf{More Sophisticated Microservice Integration.}  Instead of weighting each algorithm equally, identify what types of inputs each is best at and then weight accordingly.



\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}

\end{document}
